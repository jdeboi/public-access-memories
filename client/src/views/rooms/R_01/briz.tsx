import { PAM_URL } from "../../Gallery/functions/loadImages";

const base = `${PAM_URL}debox/briz`;

export type BrizQuadType = {
  id: number;
  label: string;
  vector: string; // title
  imgUrl: string;
  didactic: string; // short wall text
  content: React.ReactNode; // longer text (revealed on proximity)
};

export const BrizData: BrizQuadType[] = [
  {
    id: 0,
    vector: "<VECTOR 1>",
    label: "glitch(alike) prompts",
    imgUrl: `${base}/vv1.png`,
    didactic: "DALL¬∑E 2 ‚Äúglitch art‚Äù prompt, upscaled via LDSR.",

    content: (
      <>
        <p>
          In its "purest" form, glitch art is created by intentionally provoking
          system failures, && so it might help to first get oriented w/the
          systems we're aiming to disrupt. The central components in most
          popular modern AI systems are artificial neural networks. From the
          outside these are black boxes, seemingly simple functions which take
          some arbitrary data as input && return some arbitrary data as output.
          On the inside, these are multi dimensional grids /billions of
          parameters (numbers also known as "weights" and "biases") connected
          together across multiple layers. A raw neural network, is like a
          massive interwoven set of domino rallies, each domino a "neuron"
          w/it's weights and bias printed on them. In a domino rally the numbers
          printed on the domino are static + meaningless, but the parameters of
          a neuron are everything, they effect which neurons knock over (or
          "activate") the neurons in the next layer. At first, these parameters
          are randomized, but by training a neural network on troves of data,
          the weights + biases are adjusted resulting in a trained "model" which
          can work wonders.
        </p>
        <p>
          Today we've trained models which can drive cars, predict the future &&
          create art; the most popular type of neural network for generating
          images are called diffusion models. Trained on large data sets of
          images accompanied by natural language descriptions, these models can
          synthesize brand new images from any arbitrary text input. The image
          in &lt;vector-view-1&gt; was one of many i generated by submitting the
          text prompt "glitch art" to OpenAI‚Äôs DALL-E. i imagine many glitch
          artists would not consider this image a glitch in the purest sense,
          but rather a glitch-esque image or "glitch-alike‚Äù: an image w/glitch
          aesthetics created by some other means. This image was not produced by
          corrupting the data, short circuiting the neural network or otherwise
          misusing an AI model, there is no misuse or malfunction occurring
          here, the model is operating exactly as intended.
        </p>
        <p>
          Apps designed to produce glitch-alike images have been around for over
          a decade, but most of these produce images which wouldn't fool a
          trained glitch artist's üëÅÔ∏è. These new AI models, however, create
          fairly convincing glitch-alikes. This particular glitch-alike
          &lt;vector-view-1&gt; reminds me of the{" "}
          <a
            href="http://nickbriz.com/files/glitchresearch/gct-example.png"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            glitches
          </a>{" "}
          i'd get when corrupting video compression algorithms, but when i
          looked closer i noticed peculiar artifacts, not those associated with
          image/video compression, but rather artifacts unique to this AI image
          diffusion process. The macro blocks that appear when databending a
          JPEG should be uniform in size but these were not. The grid of pixels
          that become more apparent when glitching images should be perfectly
          aligned, but these were structured slightly diff'ly. In many of these
          "glitch art" prompted images the pixels almost looked as though they
          were painted by hand, the way Gerhard Richter would reproduce the
          artifacts of photography in his paintings. This is likely a reflection
          of the model's bias, i'd guess its training data contains less JPG
          glitches than it does hand drawn paintings.
        </p>
        <p>
          &lt;vector-view-1&gt; is the model's attempt at imagining (or
          "inferencing" in AI lingo) a traditional piece of "glitch art" as
          prompted. On the surface this is a "glitch-alike", but upon close
          inspection the model's bias is subtly exposed. If a glitch is a moment
          in a system that catches us off guard, && by doing so reveals aspects
          of that system that we might overlook, then perhaps this particular
          glitch-alike is a glitch after all?
        </p>
        <div>---</div>
        <div className="text-sm text-slate-600 mt-6">
          [2] ‚Äúglitch-alike‚Äù is a term coined by{" "}
          <a
            href="https://shaymoradi.com/"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            Shay Marodi
          </a>{" "}
          in{" "}
          <a
            href="https://nickbriz.com/files/glitchresearch/moradi-iman-2004-glitch-aesthetics.pdf"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            Glitch Aesthetics
          </a>
          , 2004
        </div>
        <div className="text-sm text-slate-600 mt-6">
          [3] See my{" "}
          <a
            href="http://nickbriz.work/?portfolio=Glitch-Codec-Tutorial"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            Glitch Codec Tutorial
          </a>{" "}
          for more info (v1-v2) 2010-2011 (v3) 2019
        </div>
        <div className="text-sm text-slate-600 mt-6">
          [4] typically unintended, often unexpected (though occasionally
          embraced) formal characteristics unique to each particular
          communication mediums, like the snowy static of a week TV signal, the
          poppy static of a scratched vinyl record or the DCT blocks of an
          overly compressed JPEG.
        </div>
        <div className="text-sm text-slate-600 mt-6">
          [5] Richter is a German painter well known for his photorealistic
          paintings which, being painted from photographs, embraced the
          artifacts caused by photography, like blurry images.
        </div>
      </>
    ),
  },
  {
    id: 1,
    vector: "<VECTOR 2>",
    label: "misused models",
    imgUrl: `${base}/vv2.png`,
    didactic:
      "feature-visualization (GoogLeNet), upscaled by nearest-neighbor.",

    content: (
      <>
        <p>
          Glitch art often reveals the inherent aesthetics of the digital
          medium. A JPEG of a flamingo will appear the same as a PNG or GIF of
          the same flamingo, but when these files are glitched (ex: by{" "}
          <a
            href="http://nickbriz.com/databending101/"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            databending
          </a>{" "}
          the file or hacking the compression algorithm that produced it) each
          will render entirely diff visual artifacts, reflections of the
          algorithmic logic behind each file format. How might we reveal the
          algorithmic logic behind an AI, what perspective can we gain from this
          && what sorts of new glitchy artifacts await us? Using a piece of
          technology the "wrong" way has always been a core tenet of glitch art:
          we use music software to edit images, digital imaging tools to edit
          audio && text editors to edit everything but. Applying this logic to
          AI, we might try to create images w/a model that was trained to
          produce something else. This is precisely how the image in
          &lt;vector-view-2&gt; was created: using GoogLeNet, an AI model not
          designed to create images, but instead trained to classify or
          "predict" the contents of an image.
        </p>
        <p>
          Show GoogLeNet an image of a flamingo && it will output (w/some
          numerical confidence score) that it is a "flamingo"; input an image of
          a platypus && it will tell u it's most likely a "platypus"; input an
          image of a coffee mug, && it may tell u that it's a "dumbbell"
          (especially if that mug is being held by a muscular human arm). When
          these algorithms fail, it's incredibly difficult to debug their logic,
          b/c the logic was not written in code by a programmer, we don't set
          the values of a model's parameters ourselves, these values are
          "learned" by the machine while training its neural network on large
          amounts of data.
        </p>
        <p>
          Depending on what u want the AI to "predict" or "infer", u start w/the
          best neural network{" "}
          <a
            href="https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            ‚Äúarchitecture‚Äù
          </a>{" "}
          (the particular arrangement of dominos) for the task. As mentioned
          earlier, we must first train the neural network using a large dataset,
          in this case a collection of labeled images. At first the network gets
          it all wrong, but because the training data contains the right answer
          (the label or "class") we can adjust all its internal parameters w/the
          help of an otherwise fairly simple piece of calculus known as a loss
          function. Do that enough times (which often requires quite a bit of
          processing power) && the weights + biases ascend/descend to just the
          right values. They become abstract reflections of the patterns in the
          images associated w/each label. The trained model can almost perfectly
          classify all images, generalizing beyond the images it was trained on.
        </p>
        <p>
          If machines can *see*, they see entirely diff'ly from the way we do.
          The patterns encoded in the parameters of a trained model are not
          legible to us. It's technically possible to view these weights +
          biases, but to us the values appear as random as they do before
          training. We can however test the model to see if it did in fact learn
          the right patterns, && while these AI models outperform classical hand
          written algorithms, they're not perfect. They can/do make mistakes. In
          the hopes of understanding why they error (+perhaps to also avoid
          future{" "}
          <a
            href="https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            PR nightmares
          </a>
          ), researchers at Google flipped GoogLeNet around. Rather than
          inputting image data && outputting the predicted class, "cat",
          "platypus", etc; they used it the "wrong way", inputting the
          predictions && returning an image. For the researchers this was a way
          of visualizing the patterns identified by the model, the combination
          of pixels which would score the highest for any given class: what u
          see in &lt;vector-view-2&gt; is the result of inputting the maximum
          confidence score for the ‚Äúflamingo‚Äù class, in a sense, this image is
          the most flamingo-like possible flamingo. This abstract image might
          not look like a flamingo to u, but it is in fact the purest expression
          of a "flamingo" the machine can "imagine". The visual artifacts in
          this image is one example of what spills out of the "black boxes" when
          we crack them open, they begin to hint at the patterns this model
          identified once trained, revealing an otherwise invisible aspect of
          the system.
        </p>
        <div>---</div>
        <div className="text-sm text-slate-600 mt-6">
          [6] See{" "}
          <a
            href="https://beyondresolution.info/A-Vernacular-of-File-Formats"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            A Vernacular of File Formats
          </a>
          , 2009-2010, by{" "}
          <a
            href="https://beyondresolution.info/"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            Rosa Menkman
          </a>{" "}
          for examples
        </div>
        <div className="text-sm text-slate-600 mt-6">
          [7]{" "}
          <a
            href="https://arxiv.org/pdf/1409.4842.pdf"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            GoogLeNet
          </a>{" "}
          was an deep neural network created by researchers at Google in 2014.
          It was trained on the{" "}
          <a
            href="https://www.image-net.org/"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            ImageNet
          </a>{" "}
          at creating the best image classifiers. It{" "}
          <a
            href="https://www.image-net.org/challenges/LSVRC/2014/results.php"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            won many of the challenges
          </a>{" "}
          that year.
        </div>
        <div className="text-sm text-slate-600 mt-6">
          [8] One popular example is the{" "}
          <a
            href="https://cocodataset.org/#home"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            COCO dataset
          </a>
        </div>
        <div className="text-sm text-slate-600 mt-6">
          [9] The most influential being{" "}
          <a
            href="https://en.wikipedia.org/wiki/Gradient_descent"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            gradient descent
          </a>
          .
        </div>
      </>
    ),
  },

  {
    id: 2,
    vector: "<VECTOR 3>",
    label: "de-optimization",
    imgUrl: `${base}/vv3.png`,
    didactic: "Stable Diffusion 1.5 with atypical sampling (R-ESRGAN upscale).",
    content: (
      <>
        <p>
          As explained in &lt;VECTOR 1&gt;, the majority of AI generated images
          today are created using diffusion models, not hacked image
          classifiers. While it is *technically* possible to setup+train ur own
          diffusion model from scratch, the vast majority of AI generated images
          today are being produced w/user facing applications created/controlled
          by companies like{" "}
          <a
            href="https://www.midjourney.com/"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            Midjourney
          </a>{" "}
          &&{" "}
          <a
            href="https://openai.com/dall-e-2"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            OpenAI's DALL-E
          </a>{" "}
          produce &lt;vector-view-1&gt;). While the ease of use afforded by
          these apps has in many ways democratized access to these algorithms,
          for the glitch artist this convenience may come at a cost. To
          re:phrase Curt Clonginger:
        </p>
        <blockquote>
          "the danger of [midjourney and DALL-E] is not the threat that they may
          wind up archiving and owning all the 'content' I produce, or that they
          are currently getting rich of the content I produce [or that they
          scraped much of our work without permission to train the algorithms to
          produce such content], but that they control the parameters within
          which I produce 'my original' content [...] we should be asking
          ourselves, who are the meta-producers? who produce the contexts
          surrounding 'creative' prosumer productions? who produce the tools
          that suggest the proper 'way' in which [artists] are to produce?"
        </blockquote>
        <p>
          When a user enters their text prompt into an app running a diffusion
          model, like DALL-E, they must typically wait for a few seconds before
          seeing the resulting image[s]. While they wait they are presented w/a
          progress bar, but behind the scenes a much more interesting
          visualization is taking place. These diffusion models take their name
          from a key aspect of this process: each image begins w/
          <a
            href="https://nickbriz.com/files/glitchresearch/ai-glitch/3.jpeg"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            Gaussian noise
          </a>
          , a *somewhat* randomized matrix of pixels, then through an
          algorithmic process, like a{" "}
          <a
            href="https://arxiv.org/pdf/2011.13456.pdf"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            stochastic differential equation
          </a>
          , or SDE, the gaussian noise is "de-noised" && smoothly transformed
          into the image prompted by the user. This denoising process is called
          "sampling". One of the ways tools like DALL-E "suggest the proper
          'way' in which artists are to produce" (Cloninger) is by deciding for
          us what the most optimized settings for producing the ‚Äúhighest
          quality‚Äù image might be, like which sampling method to use (SDE is one
          of many) && how many times, or "steps", to run it. Glitch artists,
          however, often prefer to intentionally de-optimize an application's
          settings so that it might perform "poorly", for ex: rendering overly
          compressed video to invoke video artifacts.
        </p>
        <p>
          Trained AI models are already black boxes, but these functions become
          even more opaque when interfaced w/via these user-facing AI "apps". By
          depending on these corporate interfaces, we rob ourselves of potential
          creative misuses of the code behind the apps. But our options are not
          strictly binary, there's a gradient of modes between the push-button
          prompt boxes of DALL-E used in &lt;VECTOR 1&gt; && de-constructing ur
          own neural network in python code like the researchers ref‚Äôd in
          &lt;VECTOR&gt;. By embracing open source alternatives--like{" "}
          <a
            href="https://huggingface.co/docs/diffusers/v0.14.0/en/api/pipelines/stable_diffusion/overview"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            Stable Diffusion
          </a>
          , a collaboratively trained model--we can have our app && glitch it
          too. B/c it's open source, Stable Diffusion is not only freely
          available, but the community has created a number of variations +
          modes of interfacing w/it. Most of the graphical apps created by the
          community (ex:{" "}
          <a
            href="https://github.com/AUTOMATIC1111/stable-diffusion-webui"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            A1111
          </a>
          ) provide a much wider degree of control than Midjourney or DALL-E &&
          if u setup these apps on ur own computer, having access to the code
          means u can experiment/hack it any way u like.
        </p>
        <p>
          &lt;vector-view-3&gt; was created using the Stable Diffusion model
          v1.5 w/DPM (Diffusion Probabilistic Model Sampling) as the sampling
          method. There's lots of{" "}
          <a
            href="https://twitter.com/iScienceLuvr/status/1564847717066559488?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1564847717066559488%7Ctwgr%5Ea7e0f6564df9c82ad5d8e3ef5683d95173adacaf%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fwandb.ai%2Fagatamlyn%2Fbasic-intro%2Freports%2FStable-Diffusion-and-the-Samplers-Mystery--VmlldzoyNTc4MDky"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            discussion
          </a>{" "}
          around what the most optimized combination of sampling methods + steps
          + other settings for achieving ‚Äúhigh quality‚Äù results, ignoring these
          recommendations almost always guarantees images saturated w/digital
          artifacts unique to the model && the particular sampling method
          chosen. Just as the diff compression algorithms used to create JPGs or
          PNGs glitch different when corrupted, b/c the diff sampling methods
          approach the denoising process diff'ly, they each "glitch" diff'ly
          when de-optimized.
        </p>
        <div>---</div>
        <div className="text-sm text-slate-600 mt-6">
          [10] If u‚Äôre interested in this glitch &lt;VECTOR&gt; i‚Äôd suggest
          checking out Google‚Äôs{" "}
          <a
            href="https://distill.pub/2017/feature-visualization/"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            research
          </a>{" "}
          as well as experimenting w/Tim Sainburg‚Äôs{" "}
          <a
            href="https://colab.research.google.com/github/timsainb/tensorflow-2-feature-visualization-notebooks/blob/master/0.0-Visualizing-classes.ipynb#scrollTo=5GgDpckAV2lK"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            python code
          </a>{" "}
          implementation.
        </div>
        <div className="text-sm text-slate-600 mt-6">
          [11] This remixed quote comes from his 2009 essay{" "}
          <a
            href="http://lab404.com/articles/commodify_your_consumption.pdf"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            "Commodify Your Consumption: Tactical Surfing / Wakes of Resistance"
          </a>{" "}
          where he applies Michel de Certeau's theoretical framework to the
          internet art "surf clubs" of the time.
        </div>
        <div className="text-sm text-slate-600 mt-6">
          [12] Likely the most popular collection of these community tweaked
          models can be found at{" "}
          <a
            href="https://civitai.com/models"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            https://civitai.com/models
          </a>
        </div>
      </>
    ),
  },
  {
    id: 3,
    vector: "<VECTOR 4>",
    label: "unexpected output",
    imgUrl: `${base}/vv4.png`,
    didactic: "DALL¬∑E 2 ‚Äòa human with three eyes,‚Äô LDSR upscale.",
    content: (
      <>
        <p>
          While keeping Clonginger's re:phrased warning in mind, i don't think
          it's necessary to write ur own code in python or setup/run a local
          copy of Stable Diffusion on ur own machine to glitch AI. It is
          possible to instigate "authentic" glitches (rather than simply
          glitch-alikes) by simply prompting the system through the provided
          interface, the trick is to avoid specifically prompting it for a
          "glitch" (or databending, datamoshing, pixel-bleeding, compression
          artifacts, etc). What if we prompt it w/gibberish text? What if we
          insert a prompt written in uncommon unicode characters, emojis or l33t
          speak? The better we understand these systems && the process occurring
          behind the scenes, the more likely we might conjure prompts capable of
          casting glitchy spells on the model. For example, we now know that
          diffusion models start by creating an image of colorful static known
          as "Gaussian noise", before de-noising it a number of times to produce
          an image which matches ur prompt. So what happens if we prompt it to
          make "Gaussian noise"?
        </p>
        <p>
          Diffusion models are trained on an immense dataset of
          image/text-description pairs. These text descriptions are broken up
          into "tokens", which are often small words or word fragments
          ("graffiti", "pizza", "cat", "th", "ly", "ing" are all tokens found in
          Stable Diffusion's{" "}
          <a
            href="https://huggingface.co/runwayml/stable-diffusion-v1-5/raw/main/tokenizer/vocab.json"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            vocabulary
          </a>
          ). In the case of Large Language Models like ChatGPT, both our input
          and its output are broken down into + build up from a series of
          tokens. The tokens in a model's vocabulary are a reflection of the
          most common patterns of text found in its training data, so it's
          reasonable to expect these to mirror the most commonly used words.
          Which is why researchers (inspired by the Google researchers ref'd in
          &lt;VECTOR&gt;) where surprised to find groupings of{" "}
          <a
            href="https://www.alignmentforum.org/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            "anomalous tokens"
          </a>
          , like "?????-?????-" && "rawdownloadcloneembedreportprint". What was
          even stranger was that, despite being in the model's vocabulary, GPT
          could not seem to output these tokens. When simply prompted to repeat
          them, it would often re:in strange + unexpected ways, outputting
          things like "They're not going to be happy about this" && "You're a
          fucking idiot." As a result, these unusual tokens have now been dubbed{" "}
          <a
            href="https://www.alignmentforum.org/posts/8viQEp8KBg2QSW4Yc/solidgoldmagikarp-iii-glitch-token-archaeology"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            "glitch tokens"
          </a>
          . Leaning further into our understanding of these systems, how might
          we use (or misuse) specific tokens in a diffusion model's vocabulary
          to instigate unexpected results?
        </p>
        <p>
          Anticipating the unexpected can be a tricky practice. When dealing
          with new/unfamiliar technology, it's important for the glitch artist
          to dispel any preconceived notions of what a glitch
          looks/sounds/tastes/smells/feels like. When i co-organized the first
          GLI.TC/H conference/festival in 2010 most of the glitch work shown
          were colorful/saturated pixelated blooms/moshes, but by the third
          iteration, GLI.TC/H 2112, one of the most popular types of glitches
          were black/white unconstrained jaggedy chars: a type of text-based
          glitch known as "zalgo text", best exemplified (imo) by the social
          media hacks of artist glitchr. As it turns out, glitching social media
          platforms results in an entirely diff set of aesthetic artifacts than
          glitching JPGs or MOVs. When an AI model doesn't perform as
          *intended*, what sorts of artifacts appear?
        </p>
        <p>
          One incredibly common{" "}
          <a
            href="https://twitter.com/ck7even/status/1627668199620632580"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            "bug"
          </a>{" "}
          present in many of these diffusion models were/are{" "}
          <a
            href="https://www.newyorker.com/culture/rabbit-holes/the-uncanny-failures-of-ai-generated-hands"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            "nightmarish"
          </a>{" "}
          human hands. The image in &lt;vector-view-4&gt; was created by
          prompting DALL-E with "a human with three eyes", the AI instead
          produced an image of a human w/two eyes && 8 fingers. Not only did it
          "fail" at outputting the requested image, it decided to create an
          image that emphasized these hands, positioning them on either side of
          the generated person's face. These sorts of glitches are, in a way,
          manifestations of the model's inherent bias (what AI researchers
          prefer to call "hallucinations"). The model's training data likely
          contained much more information about human faces than it did human
          hands. But it would be a mistake to ascribe the "bias" solely to the
          machine. All technology are biased, but this is b/c they are made by
          people && people are bias. The model's "bias" is really a reflection
          of the individuals who designed the system as well as the individuals
          who collected/curated the training data && the bias of all the
          individuals they scraped+mined that data from. But this "glitch" also
          points to another bias, that of the viewer. Safe to say most humans
          have 10 fingers, but do we all? What does it say about our own bias if
          we categorize this image as "abnormal" && this moment as a "glitch"?
        </p>
        <div>---</div>
        <div className="text-sm text-slate-600 mt-6">
          [13] Reference Domenico‚Äôs experiments
        </div>
        <div className="text-sm text-slate-600 mt-6">
          [14] Which is precisely what Eryk Salvaggio did in{" "}
          <a
            href="https://twitter.com/e_salvaggio/status/1623539155106951168?t=pIpNn2RGKMEmVT8Ct5Gj1A&s=19"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            this series of experiments
          </a>
        </div>
        <div className="text-sm text-slate-600 mt-6">
          [15] Stable Diffusion was trained on the{" "}
          <a
            href="https://laion.ai/blog/laion-5b/"
            target="_blank"
            style={{ color: "blue" }}
            rel="noopener noreferrer"
          >
            LAION-5B
          </a>{" "}
          dataset for example{" "}
        </div>
        <div className="text-sm text-slate-600 mt-6">
          [16] At the time of this writing, the most popular diffusion models
          still distort (on occasion) human hands, but ‚Äúbugs‚Äù like these are
          quickly addressed, what glitch artists embrace as a ‚Äúfeature‚Äù often
          disappear as software ‚Äúupdates‚Äù. This is why glitch artists often
          avoid updating their software && even keep old computers/hardware on
          hand.
        </div>
      </>
    ),
  },
  //   { id: 4, label: "<VECTOR 5>in-progress", content: <>tetesst</> },
];
